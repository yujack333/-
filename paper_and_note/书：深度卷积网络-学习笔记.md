 ## 第一章：
 预测学习（predictive learning）？ 三巨头均对预测学习投入研究，会成为AI的下一次飞跃
### 泛化能力
### 特征提取的能力
深度神经网络可以自动发现特征，而且可以自动建立逐层的抽象结构：低层为低级的片段特征，高层为高级的语义特征。
### 人与深度学习为代表的AI的相同和不同：
　　规律可以分为两种：一种是逻辑规律，一种是统计规律。人既有逻辑规律也有统计规律，人得统计规律可以理解为直觉和感觉。
深度学习模型更多是通过数据得到统计规律，因此深度学习模型有时也会犯低级错误。在这个意义上，深度学习和我们的生物神经网络
的直觉有某种深层次的相似性。所以可以将统计规律和逻辑规律结合起来，例如ａｌｐｈａｇｏ在利用深度学习之余还加上了逻辑性很强的
蒙特卡洛树搜索。

　　当然深度学习学习出来的统计规律的强大还是超出我们的想象的。
## 第二-第五章
### 网络debug：
- 1.初始化参w不能初始化为相同值，不然之后的更新对于同层的神经元而言就是一样的。
- 2.不加非线性激活函数，则多层网络就相当于一层线性映射的作用。
- 3.relu可以解决sigmoid和tanh引起的梯度消失
- 4.梯度爆炸往往是由于初始化的W量级太大导致输出的量级太大，从而loss很大，导致导数出现NAN即梯度爆炸。
- 5.欠拟合：网络不能很好的拟合数据的映射（x->y）：a.有可能是网络的capacity不够，b.超参数设置不当（学习率过大或者过小，初始化方法不佳网络架构不合适，损失函数不能刻画输出和label之间的差距等等）, c.数据问题，数据的噪声过大，分布过于极端

### 网络压缩：
对于对速度有要求的应用（自动驾驶等等），网络压缩的总要性不言而喻。我们可以容忍用很多机器很多时间去训练一个模型，但在实际部署（deploy）时，我们希望模型的运行速度尽可能的快。这里总结一些网络压缩的方法。

- 1.知识蒸馏：

　　开山之作：[Hinton发表在NIPS2014文章:Distilling the Knowledge in a Neural Network](https://github.com/yujack333/study_things/blob/master/paper_and_note/1503.02Distilling%20the%20Knowledge%20in%20a%20Neural%20Network.pdf)
　　方法：先训练一个大的模型（这个模型的capacity很大，可以很好的拟合数据，cumbersome mode），再设计一个小模型（这个模型的capacity比之前的小）。在小模型的训练时，两个loss的加权和为最后的loss。第一个loss是常规的分类loss（这里用分类来举例，其实可以把这个框架扩展到其它任务中去），第二个loss则是用大模型的输出作为标签与小模型的输出算。之所以用大模型的输出作为标签来算loss，是因为大模型的输出是有knowledge在里面的，比如在预测minist数据集中的2时，2的probability是0.999，3的probability是0.0001,7的probability是0.000001。虽然2和7的p都很小，但是在模型看来，这个图片更可能是3也不可能是7。这就是很有用的信息，当然如果任然用softmax+cross-entropy这一点差别是不能被体现出来的。文章对于softmax函数做了一些改进：这里引入了一个参数T，这个参数用来控制网络输出logits的平滑程度，也就是说T越大，那么logits之间的差距也就越平滑。就之前的两个loss的加权参数而言，由于第二个loss的导数的大小会乘以1/T²，所以这个loss要先乘T²。
  
  ![](/pic/distilling_1.png)
　　此外，文章还提出一种新的集成模型（Ensembles of Models）方法，包括一个通用模型（Generalist Model）和多个专用模型（Specialist Models），其中，专用模型用来对那些通用模型无法区分的细粒度（Fine-grained）类别的图像进行区分，这里专用模型就是利用通用模型来蒸馏出来的。

- 1.网络结构的压缩-分解：

　　mobile-net：[google 发布的方法，将卷积操作分解为depth-wise和point-wise两步来减少运算量](https://github.com/yujack333/study_things/blob/master/paper_and_note/MobileNets_note.md)
  
　　shuffle-net:[face++ 发布的方法，将卷积划分为group-conv来减少计算量，并采用resnet连接结构](https://www.cnblogs.com/heguanyou/p/8087422.html)
  
　　shuffle-net-v2:[face++ 发布的方法，提出不能单用FLOPs来衡量网络的运行速度。FLOPs相同的网络运行速度可能有很大的差别。提出影响网络运行速度的因素，并重新设计网络架构](https://blog.csdn.net/u014380165/article/details/81322175)

   
