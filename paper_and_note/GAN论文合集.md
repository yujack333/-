* [1.GAN](#1)
* [2.DC-GAN](#2)
* [3.W-GAN](#3)

<h2 id="1">1.GAN</h2>

- [论文地址](https://arxiv.org/pdf/1406.2661.pdf)

------
### 总结
GAN模型包括了两个模块：
- 1.G：生成器。用来生成数据（这里的数据可以是图片，序列等等，这个生成模型生成的数据是为了拟合训练集数据里的分布）
- 2.D：判别器。用来判别生成器产生的数据是否符合训练集数据的分布（就像一个teacher）

G和D可以看成是相互竞争也可以看成是相互进步，他们在玩一个min max的游戏。G希望生成的数据可以让D认为符合训练集分布，
D希望可以判断出G生成的数据不符合训练集的分布。这样最后会达成一个纳什均衡，也就是说：G产生数据的分布和训练集的数据分布几乎一样，
而D无法判断一个数据是训练数据集的还是G产生的。

------
### 论文细节
- 1.loss function：

![](/pic/1.png)

- 2.loss function的一些问题

![](/pic/G_loss.png)

可以看出在早期，G产生的数据可以被D很容易的判断出来，这样导致![](/pic/GAN_2.png)(saturate),也就是说梯度消失导致G无法被训练。
所以可以将原先的最小化![](/pic/GAN_2.png)改为最大化![](/pic/GAN_3.png)。

- 3.GAN算法

算法步骤：

- 1.在G中生成m个样本
- 2.在真实数据中采样m个样本
- 3.利用这些样本来训练D
- 4.重复1-3 K次
- 5.重复1-4 n次

![](/pic/GAN_algo.png)

-----
<h2 id="2">2.DC-GAN</h2>

- [论文地址](https://arxiv.org/pdf/1511.06434.pdf)

-----
### 总结

在GAN中使用卷积操作时会导致训练很不稳定，从而使得训练不成功。作者通过尝试不同的模型架构找到了一族架构，
这族架构可以设计很深且产生高分辨率的模型。这族架构在多个数据集上都可以训练出稳定的结果。

主要的架构上的改进为：
- 1.用全卷积。利用卷积中stride来down sample和up sample。让卷积自己去学习空间上的降采样和上采样。
- 2.移除全连接层
- 3.加batch normlization。生成器的输出层不加，判别器的输出层不加。
- 4.激活函数。生成器用relu，输出层用tanh。判别器用leacky relu。

-----
<h2 id="3">3.W-GAN</h2>

- [论文地址]（https://arxiv.org/pdf/1701.07875.pdf)

- [一个很好的总结](https://zhuanlan.zhihu.com/p/25071913)

-----
### 总结
原始的GAN的最大的问题就是训练不稳定。WGAN就GAN得训练不稳定给出了理论上的解释，并提出来新的loss来解决这个问题。
论文中提出：训练的不稳定主要是因为：当D训练得很好时G的loss不能很好的刻画生成数据集和真实数据集之间分布的距离。

原始GAN提出两种关于G的loss。
- 对于第一种loss：
如果D训练到最优。那么loss就等于![](/pic/WGAN1.png) 。其中JS为JS散度用来刻画两个分布之间的距离。问题就出现在这个JS散度上。
由于真实的分布和生成的分布几乎没有重叠的部分，或者说重叠的部分可以忽略不计。那么对于这样的两个分布而言，它们的JS散度为log2。
这就意味着loss无法衡量这两个分布的距离。因为loss恒为常数，所以梯度为0，导致无法训练生成器G。```总结来说：当D训练得太好的时候，
G几乎无法被优化。```

实验结果也印证了上述理论：

分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器随机初始化后重新训练。纵坐标为第一种loss下生成器的梯度大小。
可以看到随着判别器越来越好，生成器的梯度大小越来越小，趋近于0（消失）。

![](/pic/WGAN2.jpg)


- 对于第二种loss：
如果D训练到最优。那么loss就等价于![](/pic/WGAN4.png).其中KL和JS都为刻画两个分布距离的散度。这个等价最小化目标存在两个严重的问题。
第一是它需要同时最小化生成分布与真实分布之间的KL散度，同时又要最大化两者的JS散度，这在直观上是非常荒谬的，在数值上会导致梯度不稳定。
这是后面一项JS散度的问题。第二是KL散度也有问题，KL散度对于两类错的惩罚是不同的，对于生成的样本没有多样性的惩罚是微小的，对于生成了
不真实的样本的惩罚是巨大的。也就说在这样的惩罚下，生成器倾向于生成一些重复但“安全的样本”。也就是模型出现的collapse mode。

实验结果也印证了第二种loss会出现梯度不稳定的情况：

分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器随机初始化之后重新训练。纵坐标为第二种loss下生成器的梯度大小。
可以看到随着判别器越来越好，生成器的梯度越来越大，趋向于不稳定。

![](/pic/WGAN5.jpg)


wasserstein距离可以很好的刻画两个分布之间的距离，即使两个分布之间没有重叠的部分。W距离公式：![](/pic/WGAN6.png)
由于wasserstein距离中下界无法计算，所以经过一些估计和化简后整体模型的代价函数为：![](/pic/WGAN7.png)












