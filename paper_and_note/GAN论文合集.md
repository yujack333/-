* [1.GAN](#1)
* [2.DC-GAN](#2)


<h2 id="1">1.GAN</h2>

- [论文地址](https://arxiv.org/pdf/1406.2661.pdf)

------
### 总结
GAN模型包括了两个模块：
- 1.G：生成器。用来生成数据（这里的数据可以是图片，序列等等，这个生成模型生成的数据是为了拟合训练集数据里的分布）
- 2.D：判别器。用来判别生成器产生的数据是否符合训练集数据的分布（就像一个teacher）

G和D可以看成是相互竞争也可以看成是相互进步，他们在玩一个min max的游戏。G希望生成的数据可以让D认为符合训练集分布，
D希望可以判断出G生成的数据不符合训练集的分布。这样最后会达成一个纳什均衡，也就是说：G产生数据的分布和训练集的数据分布几乎一样，
而D无法判断一个数据是训练数据集的还是G产生的。

------
### 论文细节
- 1.loss function：

![](/pic/1.png)

- 2.loss function的一些问题

![](/pic/G_loss.png)

可以看出在早期，G产生的数据可以被D很容易的判断出来，这样导致![](/pic/GAN_2.png)(saturate),也就是说梯度消失导致G无法被训练。
所以可以将原先的最小化![](/pic/GAN_2.png)改为最大化![](/pic/GAN_3.png)。

- 3.GAN算法

算法步骤：

- 1.在G中生成m个样本
- 2.在真实数据中采样m个样本
- 3.利用这些样本来训练D
- 4.重复1-3 K次
- 5.重复1-4 n次

![](/pic/GAN_algo.png)

-----
<h2 id="2">2.DC-GAN</h2>

- [论文地址](https://arxiv.org/pdf/1511.06434.pdf)

-----
### 总结

在GAN中使用卷积操作时会导致训练很不稳定，从而使得训练不成功。作者通过尝试不同的模型架构找到了一族架构，
这族架构可以设计很深且产生高分辨率的模型。这族架构在多个数据集上都可以训练出稳定的结果。

主要的架构上的改进为：
- 1.用全卷积。利用卷积中stride来down sample和up sample。让卷积自己去学习空间上的降采样和上采样。
- 2.移除全连接层
- 3.加batch normlization。生成器的输出层不加，判别器的输出层不加。
- 4.激活函数。生成器用relu，输出层用tanh。判别器用leacky relu。



